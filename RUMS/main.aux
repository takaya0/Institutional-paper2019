\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Intoroduction}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Fundermetal concepts of Machine Learning}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}教師あり機械学習の具体例}{2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}単回帰分析・重回帰分析}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}線形回帰}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}過学習と正則化}{5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces $d = 4$の時の多項式回帰の過学習}}{5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces $\lambda = 700$, $d = 4$の時のLasso正則化多項式回帰}}{6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces $\lambda = 30000$, $d = 4$の時のLasso正則化多項式回帰の未学習}}{7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}ロジスティック回帰と勾配降下法}{7}\protected@file@percent }
\citation{GD}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Gradient Decent}}{8}\protected@file@percent }
\citation{MLR}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces $\alpha = 0.001$とした時の勾配降下法の結果.}}{9}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces 値の詳細}}{9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}MNISTでの実験}{9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces 数字の5}}{9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces 数字の9}}{9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces 数字の6}}{9}\protected@file@percent }
\newlabel{test}{{3.15}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces $\alpha = 0.0001$とした時の, MNSIT実験の結果}}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Deep Learning}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Neural Network}{11}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces ニューラルネットワークの図}}{12}\protected@file@percent }
\citation{GAN}
\bibcite{UAT}{1}
\bibcite{GAN}{2}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}The Universal Theorem of Neural Network}{13}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Generative Adversarial Networks}{13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}GANの定式化}{13}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Applications of GANs}{13}\protected@file@percent }
\bibcite{PGAN}{3}
\bibcite{SNGAN}{4}
\bibcite{GD}{5}
\bibcite{MLR}{6}
\bibcite{}{7}
