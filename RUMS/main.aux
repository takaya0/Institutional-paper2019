\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Intoroduction}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Fundermetal concepts of Machine Learning}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Some Examples}{2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}単回帰分析・重回帰分析}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}線形回帰}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}過学習と正則化}{5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces $d = 4$の時の多項式回帰の過学習}}{5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces $\lambda = 700$, $d = 4$の時のRidge正則化多項式回帰}}{6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces $\lambda = 30000$, $d = 4$の時のRidge正則化多項式回帰の未学習}}{7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}ロジスティック回帰と勾配降下法}{7}\protected@file@percent }
\citation{GD}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Gradient Decent}}{8}\protected@file@percent }
\citation{MLR}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces $\alpha = 0.001$とした時の勾配降下法の結果.}}{9}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces 値の詳細}}{9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}MNISTでの実験}{9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces 数字の5}}{9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces 数字の9}}{9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces 数字の6}}{9}\protected@file@percent }
\newlabel{test}{{3.15}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces $\alpha = 0.0001$とした時の, MNSIT実験の結果}}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Deep Learning}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Neural Network}{11}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces ニューラルネットワークの図}}{12}\protected@file@percent }
\citation{UAT}
\citation{DUAT}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}The Universal Theorem of Neural Network}{13}\protected@file@percent }
\citation{Adam}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}確率的最適化}{14}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Stchastic Gradient Decent}}{14}\protected@file@percent }
\citation{GAN}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Adaptive Moment Estimation}}{15}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Generative Adversarial Networks}{15}\protected@file@percent }
\citation{SNGAN}
\citation{GAN}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}GANの定式化}{16}\protected@file@percent }
\citation{GAN}
\bibcite{MLR}{1}
\newlabel{ODis}{{5.5}{17}}
\bibcite{Adam}{2}
\bibcite{UAT}{3}
\bibcite{GAN}{4}
\bibcite{PGAN}{5}
\bibcite{WGAN}{6}
\bibcite{SNGAN}{7}
\bibcite{GD}{8}
\bibcite{DUAT}{9}
\bibcite{}{10}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Training Instability of GAN}{18}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{18}\protected@file@percent }
